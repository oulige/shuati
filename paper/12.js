{"code":0,"data":[{"id":5194435,"qtype":"a","title":"以下选项能保证输入卷积网络后的feature map大小与原始图像大小相同的方法是哪一个（）","answer":"C","options":"[\"kernel_size为1，padding为1，stride为2\",\"kernel_size为2，padding为2，stride为2\",\"kernel_size为3，padding为1，stride为1\",\"kernel_size为2，padding为1，stride为2\"]","explain":"根据输出featuremap计算方法Out_size=(Origin_size+2*padding-kernel_size)/stride+1，结果向下取整","difficulty":1,"chapterId":0},{"id":5194436,"qtype":"a","title":"通过采用将mini_batch的网络输出的特征图进行减均值除方差操作来达到更快收敛速度以及更好模型效果的操作是以下哪种方法（）","answer":"A","options":"[\"BatchNormalization\",\"GroupNormalization\",\"SGD\",\"Adam\"]","explain":"BatchNormalization通过对输出特征进行减均值除方差操作来加速网络训练速度，同时能够达到一定的提升训练效果的方法，GroupNormalization则是对channel层面分group进行归一化，选项C和D则是优化方法，属于干扰选项","difficulty":1,"chapterId":0},{"id":5194437,"qtype":"a","title":"下列哪一项在神经网络中引入了非线性（）","answer":"D","options":"[\"全连接层\",\"卷积层\",\"池化层\",\"激活函数\"]","explain":"激活函数最重要的作用就是引入非线性","difficulty":1,"chapterId":0},{"id":5194438,"qtype":"a","title":"下列哪一项不属于深度学习中常用的加速器（）","answer":"B","options":"[\"Adagrad\",\"Softplus\",\"RMSprop\",\"Adam\"]","explain":"Softplus属于激活函数，其余都是常用的优化器","difficulty":1,"chapterId":0},{"id":5194439,"qtype":"a","title":"神经网络深度过深容易出现什么问题（）","answer":"A","options":"[\"梯度消失\",\"梯度爆炸\",\"模型欠拟合\",\"训练数据丢失\"]","explain":"神经网络过深最有可能出现的情况就是反向传播的过程中出现梯度消失，导致前几层网络的参数无法更新","difficulty":1,"chapterId":0},{"id":5194440,"qtype":"a","title":"下列不属于池化层作用的是哪一项（）","answer":"C","options":"[\"防止过拟合\",\"减小特征图大小\",\"缓解梯度消失\",\"减少神经网络模型的参数\"]","explain":"池化层无法缓解梯度消失，其余都是池化层的作用","difficulty":1,"chapterId":0},{"id":5194441,"qtype":"a","title":"GRU属于以下哪种神经网络（）","answer":"C","options":"[\"前馈神经网络\",\"卷积神经网络\",\"循环神经网络\",\"生成对抗网络\"]","explain":"GRU属于RNN的一种","difficulty":1,"chapterId":0},{"id":5194442,"qtype":"a","title":"TensorFlow2.0不支持以下哪个运算符（）","answer":"D","options":"[\"//\",\"@\",\"pow\",\"^\"]","explain":"TensorFlow2.0不包括^运算符","difficulty":1,"chapterId":0},{"id":5194443,"qtype":"a","title":"LSTM不包括以下哪个门（）","answer":"C","options":"[\"输入门\",\"遗忘门\",\"更新门\",\"输出门\"]","explain":"LSTM共三个门，输入门，遗忘门和输出门，更新门是GRU中的门","difficulty":1,"chapterId":0},{"id":5194444,"qtype":"a","title":"标量属于几阶张量（）","answer":"A","options":"[\"零阶张量\",\"一阶张量\",\"二阶张量\",\"三阶张量\"]","explain":"标量属于零阶张量","difficulty":1,"chapterId":0},{"id":5194445,"qtype":"a","title":"召回率（recall）的计算方法（）","answer":"D","options":"[\"(TP+TN)/(P+N)\",\"(FP+FN)/(P+N)\",\"TP/(TP+FP)\",\"TP/P\"]","explain":"ABCD分别是准确率，错误率，查准率和召回率","difficulty":1,"chapterId":0},{"id":5194446,"qtype":"a","title":"relu激活函数的计算方法（）","answer":"B","options":"[\"y = 1/(1 + e-x)\",\"y = max(0, x)\",\"y = (ex - e-x)/(ex + e-x)\"]","explain":"A选项为sigmoid激活函数，C选项为tanh激活函数","difficulty":1,"chapterId":0},{"id":5194453,"qtype":"a","title":"InceptionNet通过加入残差块来增加模型深度（）","answer":"B","options":"[\"正确\",\"错误\"]","explain":"考查ResNet与InceptionNet之间主要的结构差别，ResNet通过加入残差块来增加卷积层数学习更深层次的特征，InceptionNet则是通过在同一层中利用多个不同的kernel_size的卷积核进行学习并组合不同的featuremap来实现更宽的模型学习","difficulty":1,"chapterId":0},{"id":5194454,"qtype":"a","title":"如果数据量较小，容易发生过拟合（）","answer":"A","options":"[\"正确\",\"错误\"]","explain":"","difficulty":1,"chapterId":0},{"id":5194455,"qtype":"a","title":"模型越复杂，训练错误越低（）","answer":"A","options":"[\"正确\",\"错误\"]","explain":"","difficulty":1,"chapterId":0},{"id":5194456,"qtype":"a","title":"训练CNN时，可以对图像进行平移、缩放等预处理来提高模型泛化能力（）","answer":"A","options":"[\"正确\",\"错误\"]","explain":"对图像数据进行预处理可以一定程度上增加模型的泛化能力","difficulty":1,"chapterId":0},{"id":5194457,"qtype":"a","title":"卷积神经网络可以对一个输入完成不同种类的变换（旋转或缩放）（）","answer":"B","options":"[\"正确\",\"错误\"]","explain":"神经网络不能完成对数据的旋转缩放等变换","difficulty":1,"chapterId":0},{"id":5194458,"qtype":"a","title":"循环神经网络中存在反馈连接（）","answer":"A","options":"[\"正确\",\"错误\"]","explain":"RNN中上层输出作为输入连接到下一个神经元","difficulty":1,"chapterId":0},{"id":5194459,"qtype":"a","title":"生成对抗网络可以用于非监督学习（）","answer":"A","options":"[\"正确\",\"错误\"]","explain":"原始的生成对抗网络可以用于非监督学习，CGAN也可以用于监督学习","difficulty":1,"chapterId":0},{"id":5194460,"qtype":"a","title":"通过训练神经网络模型改变超参数（）","answer":"B","options":"[\"正确\",\"错误\"]","explain":"训练模型无法改变超参数，超参数是靠人为设置的","difficulty":1,"chapterId":0},{"id":5194461,"qtype":"a","title":"softmax通常用于多分类任务的输出层（）","answer":"A","options":"[\"正确\",\"错误\"]","explain":"softmax将输出层的值转化为概率","difficulty":1,"chapterId":0},{"id":5194462,"qtype":"a","title":"循环神经网络可以用于分类任务（）","answer":"A","options":"[\"正确\",\"错误\"]","explain":"循环神经网络可以用于舆情分析等","difficulty":1,"chapterId":0},{"id":5194463,"qtype":"a","title":"TensorFlow1.0版本的代码可以用于TensorFlow2.0版本（）","answer":"B","options":"[\"正确\",\"错误\"]","explain":"TensorFlow1.0和TensorFlow2.0代码不互通","difficulty":1,"chapterId":0},{"id":5194464,"qtype":"a","title":"GoogLeNet模型参数比19层的VGGNet模型参数更多（）","answer":"B","options":"[\"正确\",\"错误\"]","explain":"GoogLeNet通过全局平均池化代替全连接层，大大的减少了参数数量","difficulty":1,"chapterId":0},{"id":5194465,"qtype":"a","title":"人工智能的四要素分别是数据、算法、算力、场景（）","answer":"A","options":"[\"正确\",\"错误\"]","explain":"人工智能四要素分别是数据、算法、算力、场景。大数据提供数据，云计算提供算力，不同的神经网络就是不同的算法，最后场景就是AI的应用","difficulty":1,"chapterId":0},{"id":5194466,"qtype":"a","title":"以下不属于计算机视觉课程学习内容的是（）","answer":"D","options":"[\"分类\",\"分割\",\"检测\",\"聚类\"]","explain":"","difficulty":1,"chapterId":0},{"id":5194467,"qtype":"a","title":"卷积层的作用（）","answer":"D","options":"[\"降低图片的分辨率\",\"提高图像分辨率\",\"把图片变成灰度\",\"提取图片特征\"]","explain":"","difficulty":1,"chapterId":0},{"id":5194468,"qtype":"a","title":"用什么范围可以表示颜色的明暗程度，RGB分别代表什么颜色（）","answer":"D","options":"[\"范围.0~255；RGB.蓝，绿，红\",\"范围.-255~255；RGB.红，绿，蓝\",\"范围.1~255；RGB.黄，绿，蓝\",\"范围.0~255；RGB.红，绿，蓝\"]","explain":"","difficulty":1,"chapterId":0},{"id":5194469,"qtype":"a","title":"Python程序文件扩展名字是（）","answer":"C","options":"[\".pthon\",\".p\",\".py\",\".pyth\"]","explain":"","difficulty":1,"chapterId":0},{"id":5194470,"qtype":"a","title":"OpenCV用于将图像写入文件的函数是（ ）","answer":"C","options":"[\"imread()\",\"imshow()\",\"imwrite()\",\"VideoCapture()\"]","explain":"","difficulty":1,"chapterId":0},{"id":5194471,"qtype":"a","title":"OpenCV用于将图像显示的函数是（   ）","answer":"B","options":"[\"imread()\",\"imshow()\",\"imwrite()\",\"VideoCapture()\"]","explain":"","difficulty":1,"chapterId":0},{"id":5194472,"qtype":"a","title":"下列选项中，可返回2维直方图的函数是（   ）","answer":"A","options":"[\"hist()\",\"calcHist()\",\"createCLAHE()\",\"equalizeHist()\"]","explain":"","difficulty":1,"chapterId":0},{"id":5194473,"qtype":"a","title":"下列不属于AlexNet网络的亮点是（  ）","answer":"C","options":"[\"首次利用 GPU 进行网络加速训练。\",\"使用了 ReLU 激活函数，而不是传统的 Sigmoid 激活函数以及 Tanh 激活函数。\",\"首次利用 CPU 进行网络加速训练。\",\"在全连接层的前两层中使用了 Dropout 随机失活神经元操作，以减少过拟合。\"]","explain":"","difficulty":1,"chapterId":0},{"id":5194474,"qtype":"a","title":"下列不属于VGG16网络的亮点是（）","answer":"D","options":"[\"小卷积核。作者将卷积核全部替换为3x3(不再使用7x7与5x5):\",\"小池化核。相比AlexNet的3x3的池化核, VGG全部为2x2的池化核;层数更深特征图更宽。基于前两点外,由于卷积核专注于扩大通道数、池化专注于缩小宽和高,使得模型架构上更深更宽的同时,计算量的增加放缓;\",\"采用堆积的小卷积核是优于采用大的卷积核,因为多层非线性层可以增加网络深度来保证学习更复杂的模式,而且代价还比较小(参数更少)。\",\"使前馈/反馈传播算法顺利进行，结构更加简单；恒等映射增加基本不会降低网络的性能。\"]","explain":"","difficulty":1,"chapterId":0},{"id":5194475,"qtype":"a","title":"下列属于过拟合的是（）","answer":"A","options":"[\"{-*{65374a570dddd8b50f81e419dfcd1a1a.png}*-}\",\"{-*{58117a3e6e6cbce647c4e9bff8fd845b.png}*-}\",\"{-*{72801fdb9d56a0bc95488e8f2a92c02b.png}*-}\"]","explain":"","difficulty":1,"chapterId":0},{"id":5194476,"qtype":"a","title":"下列不属于网络过拟合原因的是（）","answer":"B","options":"[\"训练集的数据太少，训练集和新数据的特征分布不一致\",\"模型复杂度过低、特征量过少\",\"训练集中存在噪音。噪音大到模型过分记住了噪音的特征，反而忽略了真实的输入输出间的关系。\",\"权值学习迭代次数足够多，拟合了训练数据中的噪音和训练样例中没有代表性的特征。\"]","explain":"","difficulty":1,"chapterId":0},{"id":5194483,"qtype":"a","title":"在混淆矩阵中真实为真，预测为假的数量为（）","answer":"B","options":"[\"True Positive\",\"False Negative\",\"False Positive\",\"True Negative\"]","explain":"","difficulty":1,"chapterId":0},{"id":5205539,"qtype":"e","title":"什么是深度学习?","answer":"深度学习涉及获取大量结构化或非结构化数据，并使用复杂算法训练神经网络。它执行复杂的操作来提取隐藏的模式和特征(例如，区分猫和狗的图像)","options":"[]","explain":"","difficulty":1,"chapterId":0},{"id":5205540,"qtype":"e","title":"什么是神经网络?","answer":"神经网络复制了人类的学习方式，灵感来自于我们大脑中的神经元是如何激活的，但是比人类大脑要简单得多。\n最常见的神经网络由三个网络层组成:输入层;隐藏层(这是最重要的一层，在这里进行特征提取，并进行调整以更快地训练和更好地运行);输出层.神经网络用于深度学习算法，如CNN, RNN, GAN等。","options":"[]","explain":"","difficulty":1,"chapterId":0},{"id":5205541,"qtype":"e","title":"什么是多层感知机(MLP)?","answer":"和神经网络一样，mlp有一个输入层、一个隐藏层和一个输出层。它与具有一个或多个隐藏层的单层感知器的的结构相同。单层感知器只能对具有二进制输出 (0,1) 的线性可分类进行分类，但 MLP 可以对非线性类进行分类。\n除输入层外，其他层中的每个节点都使用非线性激活函数。输入层、传入的数据和激活函数基于所有节点和权重相加从而产生输出。MLP 使用一种称为“反向传播”的方法来优化节点的权重。在反向传播中，神经网络在损失函数的帮助下计算误差，从误差的来源向后传播此误差（调整权重以更准确地训练模型）。","options":"[]","explain":"","difficulty":1,"chapterId":0},{"id":5205542,"qtype":"e","title":"什么是数据规范化（Normalization），我们为什么需要它？","answer":"Normalization的中文翻译一般叫做“规范化”，是一种对数值的特殊函数变换方法，也就是说假设原始的某个数值是x，套上一个起到规范化作用的函数，对规范化之前的数值x进行转换，形成一个规范化后的数值。\n规范化将越来越偏的分布拉回到标准化的分布,使得激活函数的输入值落在激活函数对输入比较敏感的区域,从而使梯度变大,加快学习收敛速度,避免梯度消失的问题。\n按照规范化操作涉及对象的不同可以分为两大类：\n一类是对第L层每个神经元的激活值 进行Normalization操作，比如BatchNorm/ LayerNorm/ InstanceNorm/ GroupNorm等方法都属于这一类；\n另外一类是对神经网络中连接相邻隐层神经元之间的边上的权重进行规范化操作，比如Weight Norm就属于这一类。\n一般机器学习里看到的损失函数里面加入的对参数的的L1/L2等正则项，本质上也属于这这一类的规范化操作。\nL1正则的规范化目标是造成参数的稀疏化，就是争取达到让大量参数值取得0值的效果，而L2正则的规范化目标是有效减小原始参数值的大小。\n有了这些规范目标，通过具体的规范化手段来改变参数值，以达到避免模型过拟合的目的。","options":"[]","explain":"","difficulty":1,"chapterId":0},{"id":5205543,"qtype":"e","title":"什么是玻尔兹曼机？","answer":"最基本的深度学习模型之一是玻尔兹曼机，类似于多层感知器的简化版本。这个模型有一个可见的输入层和一个隐藏层——只是一个两层的神经网络，可以随机决定一个神经元应该打开还是关闭。节点跨层连接，但同一层的两个节点没有连接。","options":"[]","explain":"","difficulty":1,"chapterId":0},{"id":5205544,"qtype":"e","title":"激活函数在神经网络中的作用是什么？","answer":"激活函数模拟生物学中的神经元是否应该被激发。它接受输入和偏差的加权和作为任何激活函数的输入。从数学角度讲引入激活函数是为了增加神经网络模型的非线性。Sigmoid、ReLU、Tanh 都是常见的激活函数。","options":"[]","explain":"","difficulty":1,"chapterId":0},{"id":5205545,"qtype":"e","title":"什么是成本函数?","answer":"成本函数也被称为“损失”或“误差”，它是评估模型性能好坏的一种度量方法。它用于计算反向传播过程中输出层的误差。我们通过神经网络将错误向后推并在不同的训练函数中使用它。","options":"[]","explain":"","difficulty":1,"chapterId":0},{"id":5205546,"qtype":"e","title":"什么是梯度下降?","answer":"梯度下降是一种最小化成本函数或最小化误差的最优算法。目的是找到一个函数的局部全局极小值。这决定了模型应该采取的减少误差的方向。","options":"[]","explain":"","difficulty":1,"chapterId":0},{"id":5205547,"qtype":"e","title":"反向传播是什么?","answer":"这是深度学习面试中最常被问到的问题之一。\n1974年，Paul Werbos首次给出了如何训练一般网络的学习算法—back propagation。这个算法可以高效的计算每一次迭代过程中的梯度。反向传播算法是目前用来训练人工神经网络（Artificial Neural Network，ANN）的最常用且最有效的算法。其主要思想是：\n将训练集数据输入到ANN的输入层，经过隐藏层，最后达到输出层并输出结果，这是ANN的前向传播过程；\n由于ANN的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；\n在反向传播的过程中，根据误差调整各种参数的值；不断迭代上述过程，直至收敛。","options":"","explain":"","difficulty":1,"chapterId":0},{"id":5205548,"qtype":"e","title":"前馈神经网络和循环神经网络有什么区别？","answer":"前馈神经网络信号从输入到输出沿一个方向传播。没有反馈回路；网络只考虑当前输入。它无法记住以前的输入（例如 CNN）。\n循环神经网络的信号双向传播，形成一个循环网络。它考虑当前输入和先前接收到的输入，以生成层的输出，并且由于其内部存储器，它可以记住过去的数据。","options":"[]","explain":"","difficulty":1,"chapterId":0},{"id":5205549,"qtype":"e","title":"循环神经网络 (RNN) 有哪些应用？","answer":"RNN 可用于情感分析、文本挖掘等，可以解决时间序列问题，例如预测一个月或季度的股票价格。","options":"[]","explain":"","difficulty":1,"chapterId":0},{"id":5205550,"qtype":"e","title":"Softmax 和 ReLU 函数是什么？","answer":"Softmax 是一种激活函数，可生成介于 0 和 1 之间的输出。它将每个输出除以所有输出的总和，使得输出的总和等于 1。Softmax 通常用于分类任务的输出层和注意力机制的计算。\nReLU是使用最广泛的激活函数。如果 X 为正，则输出 X，否则为零。ReLU 常用于隐藏层的激活函数。","options":"[]","explain":"","difficulty":1,"chapterId":0},{"id":5205551,"qtype":"e","title":"什么是超参数？","answer":"这是另一个经常被问到的深度学习面试问题。超参数在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。因为一般情况下我们将可以根据模型自身的算法，通过数据迭代自动学习出的变量称为参数，而超参数的设置可以影响到这些参数是如何训练，所以称其为超参数。","options":"[]","explain":"","difficulty":1,"chapterId":0},{"id":5205552,"qtype":"e","title":"如果学习率设置得太低或太高会发生什么?","answer":"当学习率太低时，模型的训练将进展得非常缓慢，因为只对权重进行最小的更新。它需要多次更新才能达到最小值。如果非常小可能最终的梯度可能不会跳出局部最小值，导致训练的结果并不是最优解。\n如果学习率设置得太高，由于权重的急剧更新，这将导致损失函数出现不希望的发散行为。可能导致模型无法收敛，甚至发散(网络无法训练)。","options":"[]","explain":"","difficulty":1,"chapterId":0},{"id":5205553,"qtype":"e","title":"什么是Dropout和BN?","answer":"Dropout是一种随机删除网络中隐藏和可见单元的技术，可以以防止数据过拟合(通常删除20%内的节点)。它使收敛网络所需的迭代次数增加。\nBN是一种通过对每一层的输入进行规范化，变为平均为0，标准差为1的正态分布，从而提高神经网络性能和稳定性的技术。","options":"[]","explain":"","difficulty":1,"chapterId":0},{"id":5205554,"qtype":"e","title":"什么是过拟合和欠拟合，以及如何解决?","answer":"过拟合是指模型在训练集上表现很好，到了验证和测试阶段就很差，即模型的泛化能力很差。当模型对训练数据中的细节和噪声的学习达到对模型对新信息的执行产生不利影响的程度时，就会发生过拟合。它更可能发生在学习目标函数时具有更大灵活性的非线性模型中。样本数量太少，样本噪音干扰过大，模型复杂度过高都会产生过拟合。\n欠拟合是指模型在训练集、验证集和测试集上均表现不佳的情况。这通常发生在训练模型的数据较少且不正确的情况下。\n为了防止过拟合和欠拟合，您可以重新采样数据来估计模型的准确性(k-fold交叉验证)，并通过一个验证数据集来评估模型。","options":"[]","explain":"","difficulty":1,"chapterId":0},{"id":5205555,"qtype":"e","title":"如何在网络中初始化权值?","answer":"一般情况下都使用随机初始化权值。\n不能将所有权重初始化为0，因为这将使您的模型类似于线性模型。所有的神经元和每一层都执行相同的操作，给出相同的输出，使深层网络无用。\n随机初始化所有权重通过将权重初始化为非常接近0的值来随机分配权重。由于每个神经元执行不同的计算，它使模型具有更好的准确性。","options":"[]","explain":"","difficulty":1,"chapterId":0},{"id":5205556,"qtype":"e","title":"CNN中常见的层有哪些?","answer":"卷积层——执行卷积操作的层，创建几个更小的图片窗口来浏览数据。\n激活层-它给网络带来非线性，例如RELU将所有负像素转换为零。输出是一个经过整流的特征映射。\n池化层——池化是一种向下采样的操作，它降低了特征图的维数。\n全连通层——该层出处类别或者回归的数值。","options":"[]","explain":"","difficulty":1,"chapterId":0},{"id":5205557,"qtype":"e","title":"CNN的“池化”是什么?它是如何运作的?","answer":"池化用于减少CNN的空间维度。它执行下采样操作来降低维数，并通过在输入矩阵上滑动一个过滤器矩阵来创建一个汇集的特征映射。","options":"[]","explain":"","difficulty":1,"chapterId":0},{"id":5207851,"qtype":"b","title":"神经网络中常见的激活函数有哪些（）","answer":"BCD","options":"[\"dropout\",\"Sigmoid\",\"tanh\",\"Leaky ReLU\"]","explain":"dropout是正则的一种方法","difficulty":1,"chapterId":0},{"id":5207852,"qtype":"b","title":"以下哪些属于模型的超参数（）","answer":"ABCD","options":"[\"池化层中的步长\",\"全连接层中神经元的数量\",\"学习率\",\"卷积层中卷积核的大小\"]","explain":"模型超参是认为设定的参数，ABCD都是","difficulty":1,"chapterId":0},{"id":5207853,"qtype":"b","title":"以下哪些属于常见的深度学习开发框架（）","answer":"ABCD","options":"[\"MXNet\",\"CNTK\",\"TensorFlow\",\"Pytorch\"]","explain":"全都是","difficulty":1,"chapterId":0},{"id":5207854,"qtype":"b","title":"CNN中常用的几个模型有哪些（）","answer":"ABCDE","options":"[\"LeNet5\",\"AlexNet\",\"VGGNet\",\"ResNet\",\"InceptionNet\"]","explain":"常用网络结构","difficulty":1,"chapterId":0},{"id":5207855,"qtype":"b","title":"深度学习方法中可以采用以下哪些方法来防止过拟合（）","answer":"ABE","options":"[\"Dropout\",\"增加正则化项\",\"减少样本数量\",\"增加训练的epoch\",\"加入BatchNormalization\"]","explain":"本题考验模型优化方法，希望减少模型过拟合的可能，一个是可以通过减小模型复杂度即进行dropout来实现，同样通过加入正则化项也可以降低模型复杂度，减少样本数量无法防止过拟合，应当适当增加，增加训练的epoch只会更容易过拟合，加入BN对于防止过拟合也有帮助","difficulty":1,"chapterId":0},{"id":5207856,"qtype":"b","title":"可以通过以下哪些方式来改变参数来最小化代价函数（）","answer":"ABC","options":"[\"穷举搜索\",\"贝叶斯优化\",\"随机搜索\",\"盲目猜测\"]","explain":"","difficulty":1,"chapterId":0}],"message":"success"}
